GRID (General Robot Intelligence Development)

=========== Сенсоры ===========

def readSensors(client) -> dict:
    sensor_data["imu"] = client.getImuData()
    sensor_data['rgb'] = client.getImages("front_center",[airgen.ImageType.Scene])[0]
    sensor_data["lidar"] = client.getLidarData()
    return sensor_data

@data_collector(readSensors, time_delta=0.1)
def move_task(client, trajectory):
    client.moveOnPath(trajectory)

from grid.model.perception.vlm.moondream import MoonDream 
from grid.model.perception.segmentation.oneformer import OneFormer 
from grid.model.perception.depth.metric3d import Metric3D 

depth_model = Metric3D()
seg_model = OneFormer()
vl_model = MoonDream() 

depth_pred = depth_model.run(rgb_image.data)
seg_pred = seg_model.run(rgb_image.data, mode="panoptic")
vl_model.run(rgb_image.data, "Describe what you see")

=========== Камеры для дрон-машины в Telemetry ===========

from grid.robot.wheeled.airgen_car import AirGenCar
airgen_car_0 = AirGenCar()

import airgen
rgb_image = airgen_car_0.client.getImages("front_center", [airgen.ImageType.Scene], 0, 0)
import rerun as rr
rr.log("robot/rgb_image", rr.Image(rgb_image))

from grid.model.perception.segmentation.oneformer import OneFormer
from grid.model.perception.depth.metric3d import Metric3D

seg_model = OneFormer()
depth_model = Metric3D()

seg_map = seg_model.run(rgb_image, "panoptic")
rr.log("robot/seg", rr.SegmentationImage(seg_map))

depth_map = depth_model.run(rgb_image)
rr.log("robot/depth", rr.DepthImage(depth_map))

=========== позволяет роботизированной руке перемещаться в пространстве ===========

# Move the Robot Arm in the XY Plane
for _ in range(50):
    curr_pos = armisaxia_0.getPosition()
    while curr_pos.x_val is None or curr_pos.y_val is None:
        curr_pos = armisaxia_0.getPosition()
        time.sleep(0.1)

    curr_pos_np = np.array([curr_pos.x_val, curr_pos.y_val, curr_pos.z_val])
    direction = np.array([0.0, 0.0, 0.0])  # Set the direction vector and step delta towards the goal
    delta_pos = direction * step_size
    goal_pos = curr_pos_np + delta_pos

    # Command the arm to move in the XY direction
    armisaxia_0.moveToGoal(
        Position(delta_pos[0], delta_pos[1], 0.0),
        Orientation(goal_pos[0])
    )
    time.sleep(0.1)

# Adjust the Arm's Z Position
for _ in range(50):
    curr_pos = armisaxia_0.getPosition()
    while curr_pos.x_val is None or curr_pos.y_val is None:
        curr_pos = armisaxia_0.getPosition()
        time.sleep(0.1)

    # Calculate the required Z-axis movement to reach goal_z
    delta_pos.z = (goal_z - curr_pos.z_val) * step_size
    armisaxia_0.moveToGoal(
        Position(0.0, 0.0, delta_pos.z),
        Orientation(goal_pos[2])
    )
    time.sleep(0.1)


=========== Движение машины ===========

road = seg_gsam_0.run(image[0][0], "road")

image_new = client.getImages("back_center", [0])
rr.log("Car", rr.Image(image_new[0][0]))

car = seg_gsam_0.run(image_new[0][0], "car")

client.setCarTargetSpeed(5)
while True:
    if client.simGetVehiclePose().position.x_val <= -120:
        client.setCarTargetSpeed(0)
        break
print(client.simGetVehiclePose().position)

controls = client.getCarControls()
controls.is_manual_gear = False
controls.steering = 1.0
client.setCarControls(controls)
client.setCarTargetSpeed(2.1)
time.sleep(3)
client.setCarTargetSpeed(0)

controls.steering = 0.0
client.setCarControls(controls)
client.setCarTargetSpeed(2)
time.sleep(3)
client.setCarTargetSpeed(0)

=========== Работа Lidar Data с машиной ===========

def get_lidar_data(client):
  lidar_data = client.getLidarData()
  if len(lidar_data.point_cloud) < 3:
      print("No points received from Lidar")
  else:
      points = np.array(lidar_data.point_cloud, dtype=np.float32)
      points = np.reshape(points, (int(points.shape[0] / 3), 3))
      points_xyz = np.zeros_like(points)
      points_xyz[:, 0] = points[:, 0]
      points_xyz[:, 1] = points[:, 1]
      points_xyz[:, 2] = -points[:, 2] + 1
      print("Got LiDAR Data")
      return points_xyz

=========== Visualize the Point Cloud ===========

%matplotlib inline
import os
from grid import GRID_USER_SESSION_BLOB_DIR
import numpy as np
import matplotlib.pyplot as plt
points = get_lidar_data(client)
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
for point in points:
    ax.scatter(point[0], point[1], point[2])
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
save_path = os.path.join(GRID_USER_SESSION_BLOB_DIR, "pcl.png")
plt.savefig(save_path, bbox_inches='tight')
print("Saved!")

=========== фронтальная камера машины в Telemetry ===========


from grid.robot.wheeled.airgen_car import AirGenCar
airgen_car_0 = AirGenCar()

from grid.model.perception.segmentation.segmentation import Segmentation
seg_model_0 = Segmentation()

import rerun as rr
import airgen
import time

client = airgen_car_0.client
client.enableApiControl(True)
client.setTargetSpeed(5)
time.sleep(10)
client.setTargetSpeed(0)

image = client.getImages("front_center", [0])
rr.log("Scene", rr.Image(image[0][0]))


=========== Центральная камера машины в Telemetry (тоже) ===========

from grid.robot.wheeled.airgen_car import AirGenCar
robot = AirGenCar()

import airgen, rerun as rr
image = robot.getImage("front_center", "rgb")

from grid.utils.logger import log
log("rgb_image", image)

=========== Распознавание камерой машины шарика в Telemetry ===========

from grid.utils.logger import log
log("rgb_image", image)

from grid.model.perception.detection.owlv2 import OWLv2

det_model = OWLv2(use_local = True)

det_model.run(image.data, "ball")


=========== Same Code, Different Robots ===========

# Wheeled robot in simulation
from grid.robot.wheeled import AirGenCar

car = AirGenCar()
car.getPosition()  # Returns Position(x, y, z)
car.getOrientation()  # Returns Orientation(x, y, z, w)

# Aerial robot in real world
from grid.robot.aerial import ModalaiDrone

drone = ModalaiDrone()
drone.getPosition()  # Same method returns Position
drone.getOrientation()  # Same method returns Orientation

=========== Sensor Access ===========

# Get camera image from real robot
from grid.robot.wheeled.jetbot import JetBot

robot = JetBot()
rgb_image = robot.getImage("front_center", "rgb")

# Get camera image from simulated robot
from grid.robot.wheeled import AirGenCar

car = AirGenCar()
rgb_image = car.getImage("front_center", "rgb")


=========== moveToGPS for Aerial ===========

AerialRobot.moveToGPS(geopoint: GeoPoint, velocity: float)


=========== moveToPosition for Aerial ===========

AerialRobot.moveToPosition(position: Vector3r, velocity: float)

=========== land for Aerial ===========

AerialRobot.land()

=========== takeoff for Aerial ===========

AerialRobot.takeoff()

=========== getImage for Aerial ===========

ModalaiDrone.getImage(camera_name: str, image_type: str)

=========== moveToGPS for Aerial ===========

ModalaiDrone.moveToGPS(lat: float, lon: float, alt: float, alt_use_msl: bool)

=========== getImage for Aergen Drone ===========

AirGenDrone.getImage(camera_name: str, image_type: str)


=========== record_state for Aergen Drone ===========

AirGenDrone.record_state()

=========== moveToPosition for Aergen Drone ===========

AirGenDrone.moveToPosition(position: Position, blocking: bool)

=========== Examples for Aergen Drone ===========

>>> # move the drone to coordinate (20.0, 100.0, 30.0), no need to flip the sign of z when a complete position (x_val, y_val, z_val) is specified
    >>> drone.moveToPosition(Position(20.0, 100.0, 30.0))

=========== Image Capture for Aergen Drone ===========

# Retrieve both Scene (RGB) and Depth images from the front center camera
image_data = client.getImages(
    "front_center",
    [airgen.ImageType.Scene, airgen.ImageType.DepthPerspective]
)

# Extract the first image, which is the RGB image, and the camera's pose
rgb_image, camera_pose = image_data[0]

# Visualize the RGB image using Rerun
import rerun as rr
rr.log("image", rr.Image(rgb_image))

# Extract the second image, which is the Depth image, and the camera's pose
depth_image, camera_pose = image_data[1]

# Visualize the Depth image using Rerun
import rerun as rr
rr.log("image", rr.DepthImage(depth_image))

=========== simSetCameraPose for Aergen Drone ===========

def simSetCameraPose(self, camera_name, pose, vehicle_name="", external=False):
    """
    Set the pose of a camera.

    Args:
        camera_name (str): Name of the camera
        pose (Pose): Desired position and orientation
        vehicle_name (str, optional): Associated vehicle name
        external (bool, optional): Controls an external camera
    """
    self.client.call("simSetCameraPose", str(camera_name), pose, vehicle_name, external)

import airgen
import time
import math
from grid.robot.airgen_car import AirGenCar

airgen_car_0 = AirGenCar()
client = airgen_car_0.client

def euler_to_quaternion(pitch, roll, yaw):
    # Convert Euler angles to quaternion
    pitch, roll, yaw = map(math.radians, [pitch, roll, yaw])
    cy, sy = math.cos(yaw * 0.5), math.sin(yaw * 0.5)
    cr, sr = math.cos(roll * 0.5), math.sin(roll * 0.5)
    cp, sp = math.cos(pitch * 0.5), math.sin(pitch * 0.5)
    return airgen.Quaternionr(cy * cr * cp + sy * sr * sp, cy * sr * cp - sy * cr * sp, cy * cr * sp + sy * sr * cp, sy * cr * cp - cy * sr * sp)

initial_pose = client.simGetCameraInfo("front_center").pose

test_poses = [
    airgen.Pose(airgen.Vector3r(0, 0, -10), euler_to_quaternion(0, 0, 0)),
    airgen.Pose(airgen.Vector3r(0, 0, -10), euler_to_quaternion(0, 0, 90)),
]

for pose in test_poses:
    client.simSetCameraPose("front_center", pose)
    time.sleep(2)
    client.simSetCameraPose("front_center", initial_pose)




=========== simSetCameraFov for Aergen Drone ===========

def simSetCameraFov(self, camera_name, fov_degrees, vehicle_name="", external=False):
    """
    Set the FOV of a camera.

    Args:
        camera_name (str): Name of the camera
        fov_degrees (float): FOV in degrees
        vehicle_name (str, optional): Associated vehicle name
        external (bool, optional): Controls an external camera
    """
    self.client.call("simSetCameraFov", str(camera_name), fov_degrees, vehicle_name, external)



=========== Segmentation for Aergen Drone ===========

simSetSegmentationObjectID("chair0", 20, True)

simSetSegmentationObjectID("chair[\w]*", 20, True)

client = airgen.MultirotorClient()
client.simSetSegmentationObjectID("Fire[\w]*", 255)
thermal_image = client.getImages("front_center", [airgen.ImageType.Infrared])[0][0]

r = round(0.5x + 0.5) * 255

    g = round(0.5y + 0.5) * 255

    b = round(0.5z + 0.5) * 255

x = (r / 255.0) * 2 - 1
    
   y = (g / 255.0) * 2 - 1

   z = (b / 255.0) * 2 - 1



=========== Object Detection for Aergen Drone ===========

API

client.simAddDetectionFilterMeshName(camera_name, image_type, mesh_name, vehicle_name='')

client.simClearDetectionMeshNames(camera_name, image_type, vehicle_name='')

client.simSetDetectionFilterRadius(camera_name, image_type, radius_cm, vehicle_name='')

client.simGetDetections(camera_name, image_type, vehicle_name='')

DetectionInfo:
    name = ''
    geo_point = GeoPoint()
    box2D = Box2D()
    box3D = Box3D()
    relative_pose = Pose()

=========== Barometer for Aergen Drone ===========

client = airgen.MultirotorClient()
client.getBarometerData()

<BarometerData> {   'altitude': 121.27217102050781,
    'pressure': 99876.0078125,
    'qnh': 1013.25,
    'time_stamp': 1721339963514664704}

=========== GPS for Aergen Drone ===========

client = airgen.MultirotorClient()
gps_data = client.getGpsData()
latitude = gps_data.gnss.geopoint.latitude
longitude = gps_data.gnss.geopoint.longitude
altitude = gps_data.gnss.geopoint.altitude

=========== IMU for Aergen Drone ===========

client = airgen.MultirotorClient()
imu_data = client.getImuData()

<ImuData>
{   'angular_velocity': <Vector3r> {   'x_val': 0.0007983481627888978,
    'y_val': 0.000933181494474411,
    'z_val': -0.0007887388928793371},
    'linear_acceleration': <Vector3r> {   'x_val': -0.11297493427991867,
    'y_val': 0.11055465042591095,
    'z_val': -9.841029167175293},
    'orientation': <Quaternionr> {   'w_val': -4.371138828673793e-08,
    'x_val': -0.0,
    'y_val': 0.0,
    'z_val': 1.0},
    'time_stamp': 1721340023239938816}

=========== Magnetometer for Aergen Drone ===========

client = airgen.MultirotorClient()
client.getMagnetometerData()

<MagnetometerData>
{   'magnetic_field_body': <Vector3r> {   'x_val': -0.25192224979400635,
    'y_val': -0.027216637507081032,
    'z_val': 0.37283220887184143},
    'magnetic_field_covariance': [   ],
    'time_stamp': 1721340120640016640}

=========== LiDAR Configuration for Aergen Drone ===========

Parameter	Description
NumberOfChannels	Number of lasers arranged vertically
X, Y, Z	Position of the LiDAR relative to the vehicle
Roll, Pitch, Yaw	Orientation of the LiDAR relative to the vehicle
VerticalFOVUpper	Topmost orientation in degrees
VerticalFOVLower	Bottommost orientation in degrees
HorizontalFOVStart	Leftmost orientation in degrees
HorizontalFOVEnd	Rightmost orientation in degrees

=========== Accessing LiDAR Data for Aergen Drone ===========

client = airgen.MultirotorClient()

# Get LiDAR data
lidar_data = client.getLidarData()

# Extract point cloud
points = numpy.array(data.point_cloud, dtype=numpy.dtype('f4'))
points = numpy.reshape(points, (int(points.shape[0]/3), 3))

# Visualize point cloud
import rerun as rr 
rr.log('lidar/points', rr.Points3D(points))

=========== Distance Sensor Configuration for Aergen Drone ===========

Parameter	Description
MinDistance	Minimum distance the sensor can capture
MaxDistance	Maximum distance the sensor can capture
X, Y, Z	Position of the sensor relative to the vehicle
Roll, Pitch, Yaw	Orientation of the sensor relative to the vehicle

=========== Accessing Distance Data for Aergen Drone ===========

client = airgen.MultirotorClient()
distance = client.getDistanceData()


				Navigation

=========== Path Planning for Aergen Drone ===========

client = airgen.MultirotorClient()
start = airgen.Vector3r(0, 0, 5)
goal = airgen.Vector3r(10, 10, 5)
path = simPlanPath(start, goal, smooth_path=True, draw_path=True)

client = airgen.MultirotorClient()
search_radius = 50 # in meters
path = simPlanPathToRandomFreePoint(search_radius, smooth_path=True, draw_path=True)

start = airgen.Vector3r(0, 0, 5)
goal = airgen.Vector3r(10, 10, 5)
path = simPlanPath(start, goal, smooth_path=True, draw_path=True)
points = []
for waypoint in trajectory:
    points.append(
        airgen.Vector3r(waypoint["x_val"], waypoint["y_val"], waypoint["z_val"])
    )

# Move the drone along the planned path at a velocity of 5 m/s
velocity = 5.0
self.drone_client.moveOnPathAsync(points, velocity, 120, airgen.DrivetrainType.ForwardOnly, airgen.YawMode(False, 0), -1, 0).join()

=========== Scene Ground Truth for Aergen Drone ===========

client = airgen.MultirotorClient()
center_position = airgen.Vector3r(0, 0, 0) # Center position of the voxel grid
bounds = 10 # Extent in meters
resolution = 0.1 # Resolution in meters
output_file = "map.binvox" # Output file
voxel_grid = client.simCreateVoxelGrid(center_position, bounds, resolution, output_file)

=========== Object Interaction for Aergen Drone ===========

client = airgen.VehicleClient()
asset_list = client.simListAssets()
pose = client.simGetVehiclePose()
pose.position.x_val += 20
scale = airgen.Vector3r(1,1,1)
client.simSpawnObject("Cube_new", "Cube", pose, scale, physics_enabled=True)


===========  AI Layer ===========

DepthAnything
The DepthAnything class implements a wrapper for the DepthAnything model, which estimates depth maps from RGB images. The model configurations are defined based on different encoder types.

​
class DepthAnything()
​
use_local
booleandefault:"False"
If True, inference call is run on the local VM, else offloaded onto GRID-Cortex. Defaults to False.

​
def run()
​
rgbimage
np.ndarrayrequired
The input RGB image of shape 
(
M
,
N
,
3
)
(M,N,3).

​
Returns
np.ndarray
The predicted output of shape 
(
M
,
N
)
(M,N).

License
Source
This code is licensed under the Apache 2.0 License.

=========== DepthAnything v2 for Aergen Drone ===========

class DepthAnything_V2()
​
mode
stringdefault:"metric"
Flag to specify the mode of the model. Can be ‘metric’ or ‘relative’. Defaults to ‘metric’.

​
use_local
booleandefault:"False"
If True, inference call is run on the local VM, else offloaded onto GRID-Cortex. Defaults to False.

​
def run()
​
rgbimage
np.ndarrayrequired
The input RGB image of shape 
(
M
,
N
,
3
)
(M,N,3).

​
Returns
np.ndarray
The predicted depth map of shape 
(
M
,
N
)
(M,N).

=========== Metric3D  ===========

class Metric3D()
​
use_local
booleandefault:"False"
If True, inference call is run on the local VM, else offloaded onto GRID-Cortex. Defaults to False.

​
def run()
​
rgbimage
np.ndarrayrequired
The input RGB image of shape 
(
M
,
N
,
3
)
(M,N,3).

​
Returns
np.ndarray
The predicted depth map of shape 
(
M
,
N
)
(M,N).

=========== MIDAS  ===========

class MIDAS()
​
use_local
booleandefault:"False"
If True, inference call is run on the local VM, else offloaded onto GRID-Cortex. Defaults to False.

​
def run()
​
rgbimage
np.ndarrayrequired
The input RGB image of shape 
(
M
,
N
,
3
)
(M,N,3).

​
Returns
np.ndarray
The predicted depth map of shape 
(
M
,
N
)
(M,N).

=========== Sensor Data  ===========

# Example: Capturing IMU data
imu_data = car.getImuData()
imu_group.create_dataset("time_stamp", data=imu_data.time_stamp)
imu_group.create_dataset("orientation", data=[
    imu_data.orientation.w_val, 
    imu_data.orientation.x_val, 
    imu_data.orientation.y_val, 
    imu_data.orientation.z_val
])
imu_group.create_dataset("angular_velocity", data=[
    imu_data.angular_velocity.x_val, 
    imu_data.angular_velocity.y_val, 
    imu_data.angular_velocity.z_val
])
imu_group.create_dataset("linear_acceleration", data=[
    imu_data.linear_acceleration.x_val, 
    imu_data.linear_acceleration.y_val, 
    imu_data.linear_acceleration.z_val
])

=========== Image Data  ===========

# Example: Capturing image data
capture_name_map = {
    "rgb": ImageType.Scene, 
    "depth": ImageType.DepthPerspective, 
    "segmentation": ImageType.Segmentation
}
capture_types = ['rgb', 'depth', 'segmentation']
image_types = [capture_name_map[capture_type] for capture_type in capture_types]
images = car.getImages("front_center", image_types)

===========  Logging and Visualization ===========

# Example: Logging images in real-time
rr_log_airgen_image("grid", capture_name_map[capture_type], image[0])


===========  AI Grounding DINO ===========

from grid.model.perception.detection.gdino import GroundingDINO
car = AirGenCar()

# We will be capturing an image from the AirGen simulator 
# and run model inference on it.

img =  car.getImage("front_center", "rgb").data

model = GroundingDINO(use_local = False)
box, scores, labels = model.run(rgbimage=img, prompt=<prompt>)
print(box, scores, labels)

## if you want to use the model locally, set use_local=True
model = GroundingDINO(use_local = True)
box, scores, labels = model.run(rgbimage=img, prompt=<prompt>)
print(box, labels)

=========== OWLv2 AI ===========

from grid.model.perception.detection.owlv2 import OWLv2
car = AirGenCar()

# We will be capturing an image from the AirGen simulator 
# and run model inference on it.

img =  car.getImage("front_center", "rgb").data

model = OWLv2(use_local = False)
box, scores, labels = model.run(rgbimage=img, prompt=<prompt>)
print(box, scores, labels)

## if you want to use the model locally, set use_local=True
model = OWLv2(use_local = True)
box, scores, labels = model.run(rgbimage=img, prompt=<prompt>)
print(box, scores, labels)

=========== RT-DETR AI ===========

from grid.model.perception.detection.rt_detr import RT_DETR
import rerun as rr
car = AirGenCar()

# We will be capturing an image from the AirGen simulator 
# and run model inference on it.

img =  car.getImage("front_center", "rgb").data

model = RT_DETR(use_local=True)
result = model.run(input=img.copy(), confidence_threshold=0.5)
rr.log("result",rr.Image(result))







